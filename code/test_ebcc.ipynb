{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def get_acc(predictions, df_truth):\n",
    "    score = (predictions == predictions.max(axis=1, keepdims=True)).astype(float)\n",
    "    score /= score.sum(axis=1, keepdims=True)\n",
    "    return score[df_truth.item.values, df_truth.truth.values].sum() / df_truth.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_path = '../data/'\n",
    "datasets = [\n",
    "#     ('crowdscale2013/sentiment',   'senti'),\n",
    "#     ('crowdscale2013/fact_eval',   'fact'),\n",
    "    ('active-crowd-toolkit/CF',           'CF'),\n",
    "    ('active-crowd-toolkit/CF_amt',       'CF_amt'),\n",
    "    ('active-crowd-toolkit/MS',           'MS'),\n",
    "    ('active-crowd-toolkit/SP',           'SP'),\n",
    "    ('active-crowd-toolkit/SP_amt',       'SP_amt'),\n",
    "    ('active-crowd-toolkit/ZenCrowd_all', 'ZC_all'),\n",
    "    ('active-crowd-toolkit/ZenCrowd_in',  'ZC_in'),\n",
    "    ('active-crowd-toolkit/ZenCrowd_us',  'ZC_us'),\n",
    "#     ('crowd_truth_inference/d_Duck Identification',            'duck'),\n",
    "    ('crowd_truth_inference/d_jn-product',                     'product'),\n",
    "    ('crowd_truth_inference/d_sentiment',                      'senti_1k'),\n",
    "#     ('crowd_truth_inference/s4_Dog data',                      'dog'),\n",
    "    ('crowd_truth_inference/s4_Face Sentiment Identification', 'face'),\n",
    "#     ('crowd_truth_inference/s4_Relevance',                     'rel'),\n",
    "    ('crowd_truth_inference/s5_AdultContent',                  'adult'),\n",
    "    ('SpectralMethodsMeetEM/bluebird', 'bird'),\n",
    "    ('SpectralMethodsMeetEM/dog',      'dog'),\n",
    "    ('SpectralMethodsMeetEM/rte',      'rte'),\n",
    "    ('SpectralMethodsMeetEM/trec',     'trec'),\n",
    "    ('SpectralMethodsMeetEM/web',      'web')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\n",
    "        ('rbm/mniste_cs_test_dataset','MnistE'),\n",
    "        ('rbm/condind_test_dataset','CondInd'),\n",
    "        ('rbm/hs3_test_dataset','HS3'),\n",
    "        ('rbm/mixedclf_test_dataset','MixedClf'),\n",
    "        ('rbm/nnt_test_dataset','NeuralNet'),\n",
    "        ('rbm/c2_boosting_test_dataset','C-Boosting'),\n",
    "        ('rbm/c2_complement_test_dataset','C-Complement'),\n",
    "        \n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def generate_dataset_list(folder_path):\n",
    "    datasets = []\n",
    "    for item in os.listdir(folder_path):\n",
    "        if os.path.isdir(os.path.join(folder_path, item)) and item.endswith('_train_dataset'):\n",
    "            # Remove '_train_dataset' suffix and create a readable name\n",
    "            name = item[:-14]  # Remove '_train_dataset'\n",
    "            readable_name = ''.join(word.capitalize() for word in name.split('-'))\n",
    "            datasets.append((f'deem/{item}', readable_name))\n",
    "    return datasets\n",
    "\n",
    "# Usage:\n",
    "# folder_path = 'path/to/your/deem/folder'\n",
    "# datasets = generate_dataset_list(folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\n",
    "        ('deem/hs3_train_dataset','HS3'),\n",
    "        ('deem/mniste_train_dataset','MnistE'),\n",
    "        ('deem/petfinder_train_dataset','Petfinder'),\n",
    "\n",
    "    ]\n",
    "datasets = [\n",
    "    ('deem/hs3_train_dataset', 'HS3'),\n",
    "    ('deem/mniste_train_dataset', 'MnistE'),\n",
    "    ('deem/petfinder_train_dataset', 'Petfinder'),\n",
    "    ('deem/artificial-characters_train_dataset', 'ArtificialCharacters'),\n",
    "    ('deem/csgo_train_dataset', 'CSGO'),\n",
    "    ('deem/eye_movements_train_dataset', 'EyeMovements'),\n",
    "    ('deem/GesturePhaseSegmentationProcessed_train_dataset', 'GesturePhaseSegmentation'),\n",
    "    #('deem/mboosting_train_dataset', 'MBoosting'),\n",
    "    #('deem/mcomplement_train_dataset', 'MComplement'),\n",
    "    ('deem/microaggregation2_train_dataset', 'Microaggregation2'),\n",
    "    ('deem/pendigits_train_dataset', 'Pendigits'),\n",
    "    #('deem/volcanoes-b2_train_dataset', 'VolcanoesB2'),\n",
    "    ('deem/tree3k_train_dataset','Tree3K')\n",
    "]\n",
    "\n",
    "#datasets = [\n",
    "#    ('deem/tree3k_train_dataset','Tree3K')\n",
    "#]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ebcc import ebcc_vb\n",
    "data_path = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "records = []\n",
    "for dataset, abbrev in datasets:\n",
    "    df_label = pd.read_csv(data_path + dataset + '/label.csv')\n",
    "    df_label = df_label.drop_duplicates(keep='first')\n",
    "    \n",
    "    elbos = []\n",
    "    seeds = []\n",
    "    results = []\n",
    "    for _ in range(5):\n",
    "        seed = np.random.randint(1e8)\n",
    "        prediction, elbo = ebcc_vb(df_label.values, num_groups=10, seed=seed, empirical_prior=True)\n",
    "        elbos.append(elbo)\n",
    "        results.append((prediction, seed, elbo))\n",
    "        \n",
    "    prediction_ik, seed, elbo = results[np.argmax(elbos)]\n",
    "        \n",
    "    df_truth = pd.read_csv(data_path + dataset + '/truth.csv')\n",
    "    records.append((abbrev, get_acc(prediction_ik, df_truth), seed, elbo))    \n",
    "    print('%-10s %10g %10d %10g'%records[-1])\n",
    "    \n",
    "df = pd.DataFrame.from_records(records, columns=['dataset', 'accuracy', 'seed', 'elbo'])\n",
    "print(df['accuracy'].mean())\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This only takes one result with the highest elbo, then calculate the acc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rbm-python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
